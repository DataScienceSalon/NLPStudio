% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize.R
\name{tokenize}
\alias{tokenize}
\title{tokenize}
\usage{
tokenize(x, tokenUnit = "word", nGrams = NULL, lowercase = FALSE,
  removePunct = FALSE, removeNumeric = FALSE, removeNonAlpha = FALSE,
  paragraphBreak = "\\n\\n")
}
\arguments{
\item{x}{Character vector containing texts}

\item{tokenUnit}{Character string indicating type of tokens to create.
Valid values include 'word', and 'sentence'.}

\item{nGrams}{Numeric indicator of length of nGrams to create.}

\item{lowercase}{Logical. If true, text is changed to lowercase prior to tokenization.}

\item{removePunct}{Logical. If TRUE, remove all characters in the Unicode "Punctuation" [P] class. Default is FALSE.}

\item{removeNumeric}{Logical.  If TRUE, numbers are removed prior to tokenization. Default is FALSE.}
}
\description{
\code{tokenize} Creates sentence, word and character tokens. A wrapper for
the tokenize functions inte tokenizer package.
Source \url{https://cran.r-project.org/web/packages/tokenizers/tokenizers.pdf}
}

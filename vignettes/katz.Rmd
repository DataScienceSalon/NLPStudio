---
title: "Design and Implementation of a Probabilistic Word Prediction Language Model with Katz Backoff Smoothing"
author: "John James jjames@datasciencesalon.org"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  rmdformats::readthedown:
    highlight: kate
    css: ../css/rmdStyles.css
    number_sections: false
editor_options: 
  chunk_output_type: inline
---


```{r knitr_init, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
options(knitr.table.format = "html")
options(max.print=100, scipen=999, width = 800)
knitr::opts_chunk$set(echo=FALSE,
	             cache=FALSE,
               prompt=FALSE,
	             eval = TRUE,
               tidy=TRUE,
               root.dir = "..",
               fig.height = 8,
               fig.width = 20,
               comment=NA,
               message=FALSE,
               warning=FALSE)
knitr::opts_knit$set(width=100, figr.prefix = T, figr.link = T)
knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})
```

```{r libraries, cache=FALSE}
library(data.table)
library(MASS)
library(extrafont)
library(ggplot2)
library(gridExtra)
library(kfigr)
library(kableExtra)
library(NLPStudio)
```


"Prediction is very difficult, especially if it's about the ...."  

One doesn't need to know this famous quote by Nobel laureate Nils Bohr, in order to predict the next word in that sentence. That's because we have an innate sense of language, its rules, grammar, and use, such that we can probably guess, with a fairly high probability, the correct word that ends that sentence. How about this one?

"One morning, I shot an elephant in my ..."

Our Groucho Marx fans will immediately appreciate the richness, possibility, and beauty in language. And it is those qualities make word prediction such an interesting area of study. 

Our task is to explore the way in which humans perform word prediction and to formalize our intuition into a language model that assigns probability to each possible word that might complete a set of prior words. We'll examine a generative n-gram language model introduced by Slava Katz in his 1987 paper "Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer".

Our examination will assume no prior knowledge of conditional probability, Markov assumptions, maximum likelihood estimation, probability densities or the chain rule of probabilities, although some working knowledge will be useful. Hopefully, we'll be able to proceed without too much math and for those of us who wish to dive deeper, references to further reading will be provided. 

Before we dive into the design and implementation details, we'll:    
1. Define what we mean by language model     
2. Introduce N-Grams     
3. Discuss Probability Estimation    
4. Interpret the Katz Back-off Algorithm    

Then we'll dive into the design and implementation of an object oriented R-based Katz-Backoff word prediction language.

Oh, it's "pajamas", btw.  And HOW did an elephant get into his pajamas?

# What is a language model?
Wikipedia offers the following definition:

>A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability ${\displaystyle P(w_{1},\ldots ,w_{m})}$ to the whole sequence.


Language models are essential components within a range of natural language processing (NLP) applications such as Augmented and Alternative Communication (AAC), statistical machine translation, speech recognition, handwriting recognition, word-sense disambiguation, spelling correction and predictive text entry. This article describes the design and implementation of a word prediction language model using a smoothing algorithm developed by Slava Katz in his 1987 paper "Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer".

No prior knowledge of language models, maximum likelihood estimation, smoothing algorithms, conditional probabilities, Markov assumptions, etc... is required, but useful. We'll cover mo 


This work is the first phase of a series of experiments designed to determine the combination of language model types, language models, features, corpus sizes, and algorithms provide best overall performance and prediction accuracy within a *simulated* mobile device environment. The aim of this first phase is to determine which probabilistic language model feature set and corpus size provide the best combination of user performance and prediction accuracy within a typical mobile device environment.  Trigram and quadgram language models, using Modified Kneser-Ney Smoothing were implemented and systematically evaluated on training sets of various sizes. 

The remainder of this section provides an introduction to probabilistic word prediction and n-Gram language models. Section 2 surveys related work in terms of the best performing n-Gram based-models and smoothing techniques. Section 3 outlines the details of the experimental methodology including corpus analysis, preprocessing, and sampling, as well as language model implementation and parameter selection.  Section 4 outlines the results of the study and finally, section 5 summarizes the most important conclusions of the work.

## Probabilistic Word Prediction
The problem of predicting the next word, given a sentence or some history, begs the question, "what is the most probable word?", given the history. Language models assign probabilities to words and sequences of words, based upon observed frequencies in corpus of text. Theoretically, one might estimate the probability that word, ${w_n}$, is the next word in a sequence of words (${w_1}$,${w_2}$, ..., $w_{n-1}$) (or the probability of the sequence (${w_1}$,${w_2}$, ..., $w_{n}$) by counting the number of times the sequence occurs and normalizing that by the number of times the history occurs as follows:

$$P({w_1},{w_2}, ..., w_{n}) = \frac{C({w_1},{w_2}, ..., w_{n})}{C({w_1},{w_2}, ..., w_{n-1})}$$
Even with a corpus as large as the internet, new language and sequences are created all the time and it may not be possible to estimate such long sequences. Using the chain rule of probabilities, we can decompose the problem into a list of conditional probabilities as follows [@Jurafsky2016]:
$$P({w_1^n}) = P({w_1})P({w_2}|{w_1})P({w_3}|{w_1^2})...P({w_n}|{w_1^{n-1}})$$
Still one is left with estimating the probability of long sequences. 

### N-Grams
Russian mathematician Andrey Markov observed that the language production is a "memoryless" stochastical process where the conditional probability distribution of A future word depends only upon the present word [@Markov1906]. Thus one can estimate the probability of word given its entire history by calculating the probability of the word given the prior word.  Thus:
$$P({w_n}|{w_1^{n-1}}) \approx P({w_n}|{w_{n-1}})$$
The language model that estimates the probability using only the previous word is called the bigram model.  This can be generalized to the trigram model that takes into account, the previous two words and the N-gram model that considers the previous N-1 words. Thus the general equation for the N-gram approximation of the conditional probability of the next word in a sentence is [@Jurafsky2016]:
$$P({w_n}|{w_1^{n-1}}) \approx P({w_n}|{w_{n-N+1}^{n-1}})$$
To calculate these N-gram probabilities, one performs a **maximum likelihood estimation** or **MLE** as follows:
$$P({w_n}|{w_{n-N+1}^{n-1}}) = \frac{C({w_{n-N+1}^{n-1}},{w_n})}{C({w_{n-N+1}^{n-1}})}$$
Equation `r figr("mleEstimation-I", F, type="Equation")` uses the relative frequency of a particular sequence and its prefix to estimate N-gram probabilities.

### Evaluating Language Models
Assuming a researcher has the resources and time, the true measure of the performance of a language model is obtained by conducting an end-to-end **extrinsic evaluation** of the model within an application [@Jurafsky2016]. **Intrinsic evaluations**, on the other hand, provide performance metrics independent of any application. Given a training set, upon which the language model is trained, a researcher can measure the performance on an unseen test set by evaluating the degree to which the model models the test set. This can be performed by calculating the probability of the test set, but this metric is rarely used in practice. **Perplexity**,  the inverse probability of the test set, normalized by the number of words, is the metric commonly used in intrinsic evaluations and is defined as follows:
$$PP(W) = \sqrt[@N]{\frac{1}{P(w_1 w_2 ... w_N)}} = \sqrt[@N]{\displaystyle\prod_{i=1}^{N}\frac{1}{P(w_i | w_1...w_{i-1})}}$$
where a test set $W = w_1 w_2 ... w_N$ [@Jurafsky2016]. 

Multiplying all the probabilities of all the N-gram together can result in numeric underflow. Instead one works in log space, adding the log of the probabilities together, then taking the average of the sum of the log probabilities over the entire corpus.  Lastly, we take the negative of the corpus log probability and raise that to the power of 2, resulting in a positive number.  The final equation is as follows:
$$Perplexity = 2^{-{1/N}{\displaystyle\sum_{i=1}^N{log p({s_i})}}}$$

Note that perplexity calculation fails with zeros in the denominator if a word or N-gram sequence from the test set doesn't occur in the training set. The next section provides techniques for dealing with such out-of-vocabulary (OOV) words and sequences. 

### Out-of-Vocabulary (OOV) Words
Intrinsic evaluations require language models to deal with unseen words in the perplexity calculation. There are essentially two techniques for training probabilities of unseen words. The first is to choose a fixed vocabulary in advance, convert in the training set, any word that is not in the vocabulary set, to an unknown word token <UNK>, then estimate the probabilities for <UNK> from counts in the training set. The other method, should a fixed vocabulary not be available, is to convert the first occurrence of each word type in the training set to <UNK>, then train the model as normal [@Jurafsky2016].

### Smoothing
Smoothing refers to a class of techniques to deal with unseen events by adjusting the maximum likelihood estimates of low probabilities upward and high probabilities downward. This not only prevents zero probabilities, but improves the predictive accuracy of the language model overall. One of the simplest smoothing techniques is Add-k smoothing [@Jeffreys61], in which the count of each n-gram is increased by $\delta$, typically $0 < \delta \leq 1$. Good-Turing smoothing estimates the probability of unseen events by reallocating the probability mass of n-grams that occur $r + 1$ times in the training data to the n-grams that occur $r$ times [@Good1953]. Since the algorithm doesn't include the combination of higher-order models with lower-order models, it is typically used with other smoothing techniques [@Chen1998]. Jelinek-Mercer smoothing interpolates higher-order maximum likely estimate probabilities with lower-order probabilities, where the interpolation factor $\lambda_{w_{i-n+1}^{i-1}}$ is estimated for each history using held-out data [@JelMer80].  Katz smoothing extends the Good-Turing estimate by adding the combination of higher-order models with lower-order models [@Katz1987]. Witten-Bell smoothing [@Bell:1990:TC:77753] extends Jelinek-Mercer smoothing in that the nth-order smoothed model is defined recursively as a linear interpolation between the nth-order maximum likelihood model and the ($n-1$)th-order smoothed model. The interpolation parameter $\lambda_{w_{i-n+1}^{i-1}}$, is computed by counting the number of unique words that follow the history $w_{i-n+1}^{i-1}$. Absolute Discounting [@Ney1994a], ), like Jelinek-Mercer smoothing, combines higher and lower-order models; however, instead of multiplying the higher-order maximum-likelihood distribution by a factor $\lambda_{w_{i-n+1}^{i-1}}$, a discount factor $D\leq1$ is subtracted from each non-zero count. Kneser-Ney smoothing [@Kneser1995] is an extension of absolute discounting where the lower-order distribution is proportional not with the count of the n-gram, but the number of unique words that follow the n-gram in the training text. Modified Kneser-Ney enhances the discounting regime by using multiple discounts $d_1$, $d-2$, $d_{3+}$ for N-grams with counts of 1, 2 and 3 or more, respectively. 

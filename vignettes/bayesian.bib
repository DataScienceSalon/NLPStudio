Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Clyde1985,
abstract = {13.1 INTRODUCTION In Chapter 12, we considered inference in a normal linear regression model with q predictors. In many instances, the set of predictor variables X can be quite large, as one considers many potential variables and possibly transfor-mations and interactions of these variables that may be relevant to modelling the response Y. One may start with a large set to reduce chances that an im-portant predictor has been omitted, but employ variable selection to eliminate variables do not appear to be necessary and avoid over-fitting. Historically, variable selection methods, such as forwards, backwards, and stepwise selec-tion, maximum adjusted R 2 , AIC, Cp, etc., have been used, and, as is well known, these can each lead to selection of a different final model (Weisberg 1985). Other modeling decisions that may arise in practice include specify-ing the structural form of the model, including choice of transformation of the response, error distribution, or choice of functional form that relates the mean to the predictors. Decisions on how to handle " outliers " may involve multiple tests with a somewhat arbitrary cut-off for p-values or the use of " ro-bust " outlier resistant methods. Many of the modeling decisions are made conditional on previous choices, and final measures of " significance " may be questionable. While one may not be surprised that approaches for selection of a model reach different conclusions, a major problem with such analyses is that often only a " best " model and its associated summaries are presented, giving a},
author = {Clyde, Merlise},
doi = {10.1002/9780470317105},
file = {::},
isbn = {9780470317105},
pages = {1--25},
title = {{Model averaging}},
url = {https://www2.stat.duke.edu/courses/Spring05/sta244/Handouts/press.pdf https://stat.duke.edu/courses/Spring05/sta244/Handouts/press.pdf},
year = {1985}
}
@article{MaxKuhnContributionsfromJedWing2017,
author = {{Max Kuhn Contributions from Jed Wing}, Author and Weston, Steve and Williams, Andre and Keefer, Chris and Engelhardt, Allan and Cooper, Tony and Mayer, Zachary and Benesty, Michael and Lescarbeau, Reynald and Ziem, Andrew and Scrucca, Luca and Tang, Yuan and Candan, Can and Hunt, Tyler and {Max Kuhn}, Maintainer},
file = {::},
title = {{Package 'caret' Classification and Regression Training Description Misc functions for training and plotting classification and regression models}},
url = {https://cran.r-project.org/web/packages/caret/caret.pdf},
year = {2017}
}
@article{Link2006,
abstract = {Statistical thinking in wildlife biology and ecology has been profoundly influenced by the introduction of AIC (Akaike's information criterion) as a tool for model selection and as a basis for model averaging. In this paper, we advocate the Bayesian paradigm as a broader framework for multimodel inference, one in which model averaging and model selection are naturally linked, and in which the performance of AIC-based tools is naturally evaluated. Prior model weights implicitly associated with the use of AIC are seen to highly favor complex models: in some cases, all but the most highly parameterized models in the model set are virtually ignored a priori. We suggest the usefulness of the weighted BIC (Bayesian information criterion) as a computationally simple alternative to AIC, based on explicit selection of prior model probabilities rather than acceptance of default priors associated with AIC. We note, however, that both procedures are only approximate to the use of exact Bayes factors. We discuss and illustrate technical difficulties associated with Bayes factors, and suggest approaches to avoiding these difficulties in the context of model selection for a logistic regression. Our example highlights the predisposition of AIC weighting to favor complex models and suggests a need for caution in using the BIC for computing approximate posterior model weights.},
author = {Link, William A and Barker, Richard J},
file = {::},
journal = {Ecology},
keywords = {AIC,Akaike's information criterion,BIC,Bayes factors,Bayesian inference,Bayesian information criterion,Salmo trutta,model averaging,model selection},
number = {10},
pages = {2626--2635},
title = {{MODEL WEIGHTS AND THE FOUNDATIONS OF MULTIMODEL INFERENCE}},
url = {https://www.pwrc.usgs.gov/prodabs/ab10060307/6623{\_}Link.pdf},
volume = {87},
year = {2006}
}
@article{Bottolo2010,
abstract = {Implementing Bayesian variable selection for linear Gaussian regression models for analysing high dimensional data sets is of current interest in many fields. In order to make such analysis operational, we propose a new sampling algorithm based upon Evolutionary Monte Carlo and designed to work under the " large p, small n " paradigm, thus making fully Bayesian multivariate analysis feasible, for example, in genetics/genomics experiments. Two real data examples in genomics are presented, demonstrating the performance of the algorithm in a space of up to 10, 000 covariates. Finally the methodology is compared with a recently proposed search algorithms in an extensive simulation study.},
author = {Bottolo, Leonard and Richardson, Sylvia},
doi = {10.1214/10-BA523},
file = {::},
journal = {Bayesian Analysis},
keywords = {Evolutionary Monte Carlo,Fast Scan Metropolis-Hastings scheme,lin-ear Gaussian regression models,variable selection},
number = {3},
pages = {583--618},
title = {{Evolutionary Stochastic Search for Bayesian Model Exploration}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.ba/1340380542},
volume = {5},
year = {2010}
}
@misc{Flixter,
author = {Flixter},
title = {{Rotten Tomatoes: Movies | TV Shows | Movie Trailers | Reviews}},
url = {http://www.rottentomatoes.com/},
urldate = {2017-11-24}
}
@article{Raftery1994,
abstract = {We consider the problems of variable selection and accounting for model uncertainty in linear regression models. Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation of uncertainty when making inferences about quantities of interest. The complete Bayesian solution to this problem involves averaging over all possible models when making inferences about quantities of interest. This approach is often not practical. In this paper we offer two alternative approaches. First we describe a Bayesian model selection algorithm called "Occam's "Window" which involves averaging over a reduced set of models. Second, we describe a Markov chain Monte Carlo approach which directly approximates the exact solution. Both these model averaging procedures provide better predictive performance than any single model which might reasonably have been selected. In the extreme case where there are many candidate predictors but there is no relationship between any of them and the response, standard variable selection procedures often choose some subset of variables that yields a high R{\^{A}}² and a highly significant overall F value. We refer to this unfortunate phenomenon as "Freedman's Paradox" (Freedman, 1983). In this situation, Occam's {\{}vVindow{\}} usually indicates the null model as the only one to be considered, or else a small number of models including the null model, thus largely resolving the paradox.},
author = {Raftery, Adrian and Madigan, David and Hoeting, Jennifer},
file = {::},
journal = {Journal of the American Statistical Association},
keywords = {bayes factor,composition,freedman,markov chain monte carlo,model,model uncertainty,occam,posterior model probability,s paradox,s window},
number = {428},
pages = {1535--1546},
title = {{Model Selection and Accounting for Model Uncertainty in Linear Regression Models}},
url = {https://www.stat.washington.edu/raftery/Research/PDF/madigan1994.pdf},
volume = {89},
year = {1994}
}
@article{10.2307/105741,
author = {Bayes, Mr. and Price, Mr.},
issn = {02607085},
journal = {Philosophical Transactions (1683-1775)},
pages = {370--418},
publisher = {The Royal Society},
title = {{An Essay towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, F. R. S. Communicated by Mr. Price, in a Letter to John Canton, A. M. F. R. S.}},
url = {http://www.jstor.org/stable/105741},
volume = {53},
year = {1763}
}
@article{Amini,
abstract = {Bayesian model averaging has increasingly witnessed applications across an array of empirical contexts. However, the dearth of available statistical software which allows one to engage in a model averaging exercise is limited. It is common for consumers of these methods to develop their own code, which has obvious appeal. However, canned statistical software can ameliorate one's own analysis if they are not intimately familiar with the nuances of computer coding. Moreover, many researchers would prefer user ready software to mitigate the inevitable time costs that arise when hard coding an econometric estimator. To that end, this paper describes the relative merits and attractiveness of several competing packages in the statistical environment R to implement a Bayesian model averaging exercise.},
author = {Amini, Shahram M and Parmeter, Christopher F},
file = {::},
title = {{BAYESIAN MODEL AVERAGING IN R}},
url = {https://core.ac.uk/download/pdf/6494889.pdf}
}
@misc{Needham1990,
abstract = {IMDb, the world's most popular and authoritative source for movie, TV and celebrity content.},
author = {Needham, Col},
title = {{IMDb - Movies, TV and Celebrities}},
url = {http://www.imdb.com/ http://www.imdb.de/},
urldate = {2017-11-24},
year = {1990}
}
@article{FernahNdez2001,
abstract = {In contrast to a posterior analysis given a particular sampling model, posterior model probabilities in the context of model uncertainty are typically rather sensitive to the speci"cation of the prior. In particular, {\&}di!use' priors on model-speci"c parameters can lead to quite unexpected consequences. Here we focus on the practically relevant situation where we need to entertain a (large) number of sampling models and we have (or wish to use) little or no subjective prior information. We aim at providing an {\&}automatic' or {\&}benchmark' prior structure that can be used in such cases. We focus on the normal linear regression model with uncertainty in the choice of regressors. We propose a partly non-informative prior structure related to a natural conjugate g-prior speci"cation, where the amount of subjective information requested from the user is limited to the choice of a single scalar hyperparameter g H . The consequences of di!erent choices for g H are examined. We investigate theoretical properties, such as consistency of the implied Bayesian procedure. Links with classical information criteria are provided. More importantly, we examine the "nite sample implications of several choices of g H in a simulation study. The use of the MC algorithm of Madigan and York (Int. Stat. Rev. 63 (1995) 215), combined with e{\$}cient coding in Fortran, makes it feasible to conduct large simulations. In addition to posterior criteria, we shall also compare the predictive performance of di!erent priors. A classic example concerning the economics of crime will also be provided and contrasted with results in the literature. The main "ndings of the 0304-4076/01/{\$} -see front matter 2001 Elsevier Science S.A. All rights reserved. PII: S 0 3 0 4 -4 0 7 6 (0 0) 0 0 0 7 6 -2},
author = {{Fernah Ndez}, Carmen and Ley, Eduardo and Steel, Mark F J},
file = {::},
journal = {Journal of Econometrics},
pages = {381--427},
title = {{Benchmark priors for Bayesian model averaging}},
url = {https://eclass.aueb.gr/modules/document/file.php/OIK164/Fernandez Ley and Steel JE 2001.pdf},
volume = {100},
year = {2001}
}
@article{Clyde2017,
abstract = {Depends R ({\textgreater}= 3.0), Imports stats, graphics, utils, grDevices Suggests MASS, knitr, GGally, rmarkdown, roxygen2 Description Package for Bayesian Variable Selection and Model Averaging in linear models and generalized linear models using stochastic or deterministic sampling without replacement from posterior distributions. Prior distributions on coefficients are from Zellner's g-prior or mixtures of g-priors corresponding to the Zellner-Siow Cauchy Priors or the mixture of g-priors from Liang et al (2008) {\textless}DOI:10.1198/016214507000001337{\textgreater} for linear models or mixtures of g-priors in GLMs of Li and Clyde (2015) {\textless}arXiv:1503.06913{\textgreater}. Other model selection criteria include AIC, BIC and Empirical Bayes estimates of g. Sampling probabilities may be updated based on the sampled models using Sampling w/out Replacement or an efficient MCMC algorithm samples models using the BAS tree structure as an efficient hash table. Uniform priors over all models or beta-binomial prior distributions on model size are allowed, and for large p truncated priors on the model space may be used. The user may force variables to always be included. Details behind the sampling algorithm are provided in Clyde, Ghosh and Littman (2010) {\textless}DOI:10.1198/jcgs.},
author = {Clyde, Merlise},
file = {::},
title = {{Bayesian Variable Selection and Model Averaging using Bayesian Adaptive Sampling}},
url = {https://github.com/merliseclyde/BAS},
year = {2017}
}
@misc{Fritz2008,
author = {Fritz, ‎Ben},
booktitle = {wikipedia},
title = {{Box Office Mojo}},
url = {http://www.boxofficemojo.com/},
year = {2008}
}
@article{Maruyama2011,
abstract = {For the normal linear model variable selection problem, we propose se-lection criteria based on a fully Bayes formulation with a generalization of Zellner's g-prior which allows for p {\textgreater} n. A special case of the prior formu-lation is seen to yield tractable closed forms for marginal densities and Bayes factors which reveal new model evaluation characteristics of potential inter-est.},
author = {Maruyama, Yuzo and George, Edward I},
doi = {10.1214/11-AOS917},
file = {::},
journal = {The Annals of Statistics},
keywords = {62C10,62F07,62F15,Bayes factor,model selection consistency,ridge regression,singular value decomposition,variable selection},
number = {5},
pages = {2740--2765},
title = {{FULLY BAYES FACTORS WITH A GENERALIZED g-PRIOR}},
url = {http://home.csis.u-tokyo.ac.jp/{~}maruyama/files/yuzo{\_}ed{\_}aos{\_}2011.pdf},
volume = {39},
year = {2011}
}
@article{Eicher2011,
abstract = {SUMMARY Bayesian model averaging (BMA) has become widely accepted as a way of accounting for model uncertainty, notably in regression models for identifying the determinants of economic growth. To implement BMA the user must specify a prior distribution in two parts: a prior for the regression parameters and a prior over the model space. Here we address the issue of which default prior to use for BMA in linear regression. We compare 12 candidate parameter priors: the unit information prior (UIP) corresponding to the BIC or Schwarz approximation to the integrated likelihood, a proper data-dependent prior, and 10 priors considered by Fern{\'{a}}ndez et al. (Journal of Econometrics 2001; 100: 381–427). We also compare two model priors: the uniform model prior and a prior with prior expected model size 7. We compare them on the basis of cross-validated predictive performance on a well-known growth dataset and on two simulated examples from the literature. We found that the UIP with uniform model prior generally outperformed the other priors considered. It also identified the largest set of growth determinants.},
author = {Eicher, Theo S and Papageorgiou, Chris and Raftery, Adrian E},
doi = {10.1002/jae.1112},
file = {::},
journal = {J. Appl. Econ},
pages = {30--55},
title = {{JOURNAL OF APPLIED ECONOMETRICS DEFAULT PRIORS AND PREDICTIVE PERFORMANCE IN BAYESIAN MODEL AVERAGING, WITH APPLICATION TO GROWTH DETERMINANTS}},
url = {https://pdfs.semanticscholar.org/dab7/263752fa35159d3c5c90b5c3ee2cf1fcc74a.pdf},
volume = {26},
year = {2011}
}
@article{Schwarz,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0780394224},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the Dimension of a Model}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aos/1176344136 http://projecteuclid.org/euclid.aos/1176344136},
volume = {6},
year = {1978}
}
@article{Liang2008a,
abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures.},
author = {Liang, Feng and Paulo, Rui and Molina, German and Clyde, Merlise A and Berger, Jim O},
doi = {10.1198/016214507000001337},
file = {::},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {AIC,BIC,Bayesian model averaging,Cauchy,Empirical bayes,Gaussian hypergeometric functions,Model selection,Zellner-Siow priors},
number = {481},
pages = {410--423},
title = {{Mixtures of g priors for Bayesian variable selection}},
url = {https://people.eecs.berkeley.edu/{~}jordan/courses/260-spring10/readings/liang-etal.pdf},
volume = {103},
year = {2008}
}
@article{Ripley2014,
author = {Ripley, Brian},
file = {::},
title = {{Package 'MASS' Title Support Functions and Datasets for Venables and Ripley's MASS}},
url = {http://www.stats.ox.ac.uk/pub/MASS4/},
year = {2014}
}
@article{Ley2008,
abstract = {We consider the problem of variable selection in linear regression models. Bayesian model averaging has become an important tool in empirical settings with large numbers of potential regressors and relatively limited numbers of observations. We examine the effect of a variety of prior assumptions on the inference concerning model size, posterior inclusion probabilities of regressors and on predictive performance. We illustrate these issues in the context of cross-country growth regressions using three datasets with 41 to 67 potential drivers of growth and 72 to 93 observations. Finally, we recommend priors for use in this and related contexts.},
author = {Ley, Eduardo and Steel, Mark F J},
file = {::},
keywords = {Model size,Model uncertainty,O47,Posterior odds,Prediction,Prior odds,Robustness JEL Classification System C11},
title = {{M P RA On the Effect of Prior Assumptions in Bayesian Model Averaging with Applications to Growth Regression On the Effect of Prior Assumptions in Bayesian Model Averaging with Applications to Growth Regression}},
url = {http://mpra.ub.uni-muenchen.de/6773/},
year = {2008}
}
@article{Zellner1980,
abstract = {Bayesian posterior odds ratios for frequently encountered hypotheses about parameters of the normal linear multiple regression model are derived and discussed. For the particular prior distributions utilized, it is found that the posterior odds ratios can be well approximated by functions that are monotonic in usual sampling theoryF statistics. Some implications of this finding and the relation of our work to the pioneering work of Jeffreys and others are considered. Tabulations of odds ratios are provided and discussed.},
author = {Zellner, A and Siow, A},
doi = {10.1007/BF02888369},
issn = {0041-0241},
journal = {Trabajos de Estadistica Y de Investigacion Operativa},
month = {feb},
number = {1},
pages = {585--603},
title = {{Posterior odds ratios for selected regression hypotheses}},
url = {https://doi.org/10.1007/BF02888369},
volume = {31},
year = {1980}
}
@article{Drucker,
author = {Drucker, Zach},
journal = {Tufts Daily},
title = {{Trailers tease Hollywood's upcoming blockbusters and Oscar-season favorites}},
url = {http://www.tuftsdaily.com/trailers-tease-hollywood-s-upcoming-blockbusters-and-oscar-season-favorites-1.2383918}
}
@article{Raftery1997,
author = {Raftery, Adrian E and Madigan, David and Hoeting, Jennifer A},
file = {::},
journal = {Journal of American Statistical Association},
keywords = {bayes factor,markov chain monte carlo,model composition,model uncertainty,occam,posterior,s window},
number = {437},
pages = {179--191},
title = {{Bayesian Model Averaging for Linear Regression Models}},
url = {https://www.stat.washington.edu/raftery/Research/PDF/rmh1997.pdf},
volume = {92},
year = {1997}
}
@article{Wasserman1996,
author = {Wasserman, Larry and Kass, Robert E.},
file = {::},
journal = {Journal of the American Statistical Association},
number = {435},
pages = {1343--1370},
title = {{The Selection of Prior Distributions by Formal Rules}},
url = {http://mathfaculty.fullerton.edu/sbehseta/KassWasserman-JASA-1996.pdf},
volume = {91},
year = {1996}
}
@incollection{zellner1986,
author = {Zellner, Arnold},
booktitle = {Bayesian inference and decision techniques: essays in honor of Bruno de Finetti},
chapter = {15},
isbn = {0444877126},
keywords = {bayesian,regression},
pages = {233--243},
publisher = {North-Holland ; Sole distributors for the U.S.A. and Canada, Elsevier Science Pub. Co.},
title = {{On assessing prior distributions and Bayesian regression analysis with g-prior distributions}},
url = {http://www.worldcat.org/isbn/0444877126},
year = {1986}
}
@article{Raftery1988,
author = {Raftery, Adrian E.},
doi = {10.1093/biomet/75.2.223},
file = {::},
isbn = {0006-3444},
journal = {Biometrika},
number = {2},
pages = {223--228},
title = {{INFERENCE FOR THE BINOMIAL N-PARAMETER - A HIERARCHICAL BAYES APPROACH}},
url = {https://www.stat.washington.edu/raftery/Research/PDF/bka1988.pdf},
volume = {75},
year = {1988}
}
@misc{George1993,
author = {George, Edward I and Mcculloch, Robert E},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1993.10476353},
file = {::},
issn = {0162-1459},
pages = {881--889},
title = {{Variable Selection Via Gibbs Sampling}},
url = {https://www2.stat.duke.edu/courses/Spring06/sta376/Support/RegressionETC/george+mcculloch.jasa93.pdf},
volume = {88},
year = {1993}
}
@article{Ghosh2017,
abstract = {In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on P{\'{o}}lya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package in the supplement. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.07170v2},
author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
eprint = {arXiv:1507.07170v2},
file = {::},
title = {{On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression}},
url = {https://arxiv.org/pdf/1507.07170.pdf},
year = {2017}
}
@article{Leamer1978,
abstract = {Section 4.3 p. 100-110, Testing a Point-Null Hypothesis Against$\backslash$na Composite Alternative. 4.4 Weighted Likelihoods: conjugate$\backslash$npriors. Bayes factors. Diffuse priors.},
author = {Leamer, Edward E},
doi = {doi: 10.2307/2287437},
isbn = {0471015202},
issn = {00222437},
journal = {SERBIULA (sistema Librum 2.0)},
pages = {370},
pmid = {3126},
title = {{Specification Searches: Ad Hoc Inference with Nonexperimental Data}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Specification+Searches{\#}7{\%}5Cnhttp://www.anderson.ucla.edu/faculty/edward.leamer/books/specification{\_}searches/specification{\_}searches.htm},
year = {1978}
}
@article{Draper1995,
author = {Draper, David},
file = {::},
journal = {J.R. Statist. Soc. B},
keywords = {multidimensional register contrast,yi},
number = {109},
pages = {45--97},
title = {{Assessment and Propagation of Model Uncertainty}},
url = {https://classes.soe.ucsc.edu/ams206/Winter11/draper-1995-model-uncertainty.pdf},
volume = {57},
year = {1995}
}
@article{Gelman2008,
abstract = {We propose a new prior distribution for classical (nonhierarchical) lo-gistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine ap-plied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also au-tomatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting pref-erences, a small bioassay experiment, and an imputation model for a public health data set.},
author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
doi = {10.1214/08-AOAS191},
file = {::},
journal = {The Annals of Applied Statistics},
keywords = {Bayesian inference,generalized linear model,hierarchical model,least squares,linear regression,logistic regression,multilevel model,noninformative prior distribution,weakly informative prior distribution},
number = {4},
pages = {1360--1383},
title = {{A WEAKLY INFORMATIVE DEFAULT PRIOR DISTRIBUTION FOR LOGISTIC AND OTHER REGRESSION MODELS}},
url = {http://www.stat.columbia.edu/{~}gelman/research/published/priors11.pdf},
volume = {2},
year = {2008}
}
@misc{Services2014,
author = {Services, Nash Information},
booktitle = {Nash Information Services, LLC},
title = {{The Numbers: Where Data and the Movie Business Meet}},
url = {http://www.the-numbers.com/},
urldate = {2017-11-22},
year = {2014}
}
@misc{IMDbstats2015,
author = {IMDbstats},
booktitle = {IMDb.com},
pages = {14--15},
title = {{IMDb Database Statistics}},
url = {http://www.imdb.com/stats},
urldate = {2017-11-24},
volume = {009},
year = {2015}
}
@article{Hodges1987,
author = {Hodges, James S.},
file = {::},
journal = {Statistical Science},
number = {3},
pages = {259--291},
title = {{Uncertainty, Policy Analysis and Statisitcs}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.ss/1177013224},
volume = {2},
year = {1987}
}
@article{Lindley1957,
author = {Lindley, D V},
issn = {00063444},
journal = {Biometrika},
number = {1/2},
pages = {187--192},
publisher = {[Oxford University Press, Biometrika Trust]},
title = {{A Statistical Paradox}},
url = {http://www.jstor.org/stable/2333251},
volume = {44},
year = {1957}
}
